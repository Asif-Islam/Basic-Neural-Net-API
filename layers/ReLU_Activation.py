import numpy as np 
from layers import Layer

class ReLU_Activation(layer):

	def __init__(self):
		

	def forward_pass(self, X, mode):
		pass

	def backward_pass(self, mode):
		pass